{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe90f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253bc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "od.download( \n",
    "    \"https://www.kaggle.com/datasets/risangbaskoro/wlasl-processed\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57962156",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./wlasl-processed/WLASL_v0.3.json\"\n",
    "videos_folder_path = \"./wlasl-processed/videos\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2145d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for entry in data:\n",
    "    gloss = entry[\"gloss\"]\n",
    "    for instance in entry[\"instances\"]:\n",
    "        video_id = instance[\"video_id\"]\n",
    "        video_file_path = os.path.join(videos_folder_path, f\"{video_id}.mp4\")\n",
    "        rows.append({\n",
    "            \"gloss\": gloss,\n",
    "            \"video_id\": video_id,\n",
    "            \"video_file_path\": video_file_path\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dcb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516eb62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gloss</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book</td>\n",
       "      <td>69241</td>\n",
       "      <td>./wlasl-processed/videos\\69241.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>65225</td>\n",
       "      <td>./wlasl-processed/videos\\65225.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book</td>\n",
       "      <td>68011</td>\n",
       "      <td>./wlasl-processed/videos\\68011.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>68208</td>\n",
       "      <td>./wlasl-processed/videos\\68208.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book</td>\n",
       "      <td>68012</td>\n",
       "      <td>./wlasl-processed/videos\\68012.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gloss video_id                     video_file_path\n",
       "0  book    69241  ./wlasl-processed/videos\\69241.mp4\n",
       "1  book    65225  ./wlasl-processed/videos\\65225.mp4\n",
       "2  book    68011  ./wlasl-processed/videos\\68011.mp4\n",
       "3  book    68208  ./wlasl-processed/videos\\68208.mp4\n",
       "4  book    68012  ./wlasl-processed/videos\\68012.mp4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9bc5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"video_exists\"] = df[\"video_file_path\"].apply(lambda path: os.path.exists(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a905b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing videos:\n",
      "            gloss video_id                     video_file_path\n",
      "1            book    65225  ./wlasl-processed/videos\\65225.mp4\n",
      "2            book    68011  ./wlasl-processed/videos\\68011.mp4\n",
      "3            book    68208  ./wlasl-processed/videos\\68208.mp4\n",
      "4            book    68012  ./wlasl-processed/videos\\68012.mp4\n",
      "5            book    70212  ./wlasl-processed/videos\\70212.mp4\n",
      "...           ...      ...                                 ...\n",
      "21074  wheelchair    63049  ./wlasl-processed/videos\\63049.mp4\n",
      "21076     whistle    63185  ./wlasl-processed/videos\\63185.mp4\n",
      "21077     whistle    67065  ./wlasl-processed/videos\\67065.mp4\n",
      "21079     whistle    63187  ./wlasl-processed/videos\\63187.mp4\n",
      "21081     whistle    63189  ./wlasl-processed/videos\\63189.mp4\n",
      "\n",
      "[9103 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "missing_videos_df = df[~df[\"video_exists\"]]\n",
    "print(\"Missing videos:\")\n",
    "print(missing_videos_df[[\"gloss\", \"video_id\", \"video_file_path\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b210a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"video_exists\"]].drop(columns=\"video_exists\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9235b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            gloss video_id                     video_file_path\n",
      "0            book    69241  ./wlasl-processed/videos\\69241.mp4\n",
      "1            book    07069  ./wlasl-processed/videos\\07069.mp4\n",
      "2            book    07068  ./wlasl-processed/videos\\07068.mp4\n",
      "3            book    07070  ./wlasl-processed/videos\\07070.mp4\n",
      "4            book    07099  ./wlasl-processed/videos\\07099.mp4\n",
      "...           ...      ...                                 ...\n",
      "11975  wheelchair    63047  ./wlasl-processed/videos\\63047.mp4\n",
      "11976  wheelchair    63050  ./wlasl-processed/videos\\63050.mp4\n",
      "11977     whistle    63186  ./wlasl-processed/videos\\63186.mp4\n",
      "11978     whistle    63188  ./wlasl-processed/videos\\63188.mp4\n",
      "11979     whistle    63190  ./wlasl-processed/videos\\63190.mp4\n",
      "\n",
      "[11980 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7557f83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gloss              object\n",
      "video_id           object\n",
      "video_file_path    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_filtered.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b659e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringLookup layer for 'gloss'\n",
    "gloss_lookup = tf.keras.layers.StringLookup(output_mode=\"int\")\n",
    "gloss_lookup.adapt(df_filtered['gloss'].values)\n",
    "\n",
    "# Transform 'gloss' to integer labels\n",
    "df_filtered['gloss_encoded'] = gloss_lookup(df_filtered['gloss']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f919f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gloss              object\n",
      "video_id           object\n",
      "video_file_path    object\n",
      "gloss_encoded       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_filtered.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eb6da10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            gloss video_id                     video_file_path  gloss_encoded\n",
      "0            book    69241  ./wlasl-processed/videos\\69241.mp4           1030\n",
      "1            book    07069  ./wlasl-processed/videos\\07069.mp4           1030\n",
      "2            book    07068  ./wlasl-processed/videos\\07068.mp4           1030\n",
      "3            book    07070  ./wlasl-processed/videos\\07070.mp4           1030\n",
      "4            book    07099  ./wlasl-processed/videos\\07099.mp4           1030\n",
      "...           ...      ...                                 ...            ...\n",
      "11975  wheelchair    63047  ./wlasl-processed/videos\\63047.mp4           1579\n",
      "11976  wheelchair    63050  ./wlasl-processed/videos\\63050.mp4           1579\n",
      "11977     whistle    63186  ./wlasl-processed/videos\\63186.mp4           1912\n",
      "11978     whistle    63188  ./wlasl-processed/videos\\63188.mp4           1912\n",
      "11979     whistle    63190  ./wlasl-processed/videos\\63190.mp4           1912\n",
      "\n",
      "[11980 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81cc6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(video_path, frame_count=30, frame_size=(224, 224)):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = video_capture.read()\n",
    "    while success and len(frames) < frame_count:\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        frames.append(frame)\n",
    "        success, frame = video_capture.read()\n",
    "    video_capture.release()\n",
    "    frames = np.array(frames) / 255.0  # Normalize to [0, 1]\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(video_path, frame_count=30, frame_size=(224, 224)):\n",
    "    # Check if the video path is valid\n",
    "    if not isinstance(video_path, str) or not os.path.exists(video_path):\n",
    "        print(f\"Invalid video path: {video_path}\")\n",
    "        return np.zeros((frame_count, *frame_size, 3))  # Return empty frames if path is invalid\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = video_capture.read()\n",
    "    while success and len(frames) < frame_count:\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        frames.append(frame)\n",
    "        success, frame = video_capture.read()\n",
    "    video_capture.release()\n",
    "    \n",
    "    # If fewer than frame_count frames were captured, pad with zeros\n",
    "    if len(frames) < frame_count:\n",
    "        frames += [np.zeros(frame_size + (3,))] * (frame_count - len(frames))\n",
    "        \n",
    "    frames = np.array(frames) / 255.0  # Normalize to [0, 1]\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cb6aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(video_path, frame_count=30, frame_size=(224, 224)):\n",
    "    # Convert the video path from bytes to string\n",
    "    video_path = video_path.numpy().decode('utf-8')  # Decode from bytes to string\n",
    "    \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Invalid video path: {video_path}\")  # Print error if file does not exist\n",
    "        return np.zeros((frame_count, frame_size[0], frame_size[1], 3), dtype=np.float32)  # Return empty frames\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = video_capture.read()\n",
    "    while success and len(frames) < frame_count:\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        frames.append(frame)\n",
    "        success, frame = video_capture.read()\n",
    "    video_capture.release()\n",
    "    \n",
    "    # If fewer than frame_count frames, pad with zeros\n",
    "    while len(frames) < frame_count:\n",
    "        # Create a black frame\n",
    "        black_frame = np.zeros((frame_size[0], frame_size[1], 3), dtype=np.float32)  # Shape (224, 224, 3)\n",
    "        frames.append(black_frame)\n",
    "        \n",
    "    frames = np.array(frames, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a83d339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(video_path, frame_count=30, frame_size=(224, 224)):\n",
    "    # Convert the video path from bytes to string\n",
    "    video_path = video_path.numpy().decode('utf-8')\n",
    "    \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Invalid video path: {video_path}\")\n",
    "        return np.zeros((frame_count, frame_size[0], frame_size[1], 3), dtype=np.float32)\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    # More efficient frame extraction\n",
    "    frame_indices = np.linspace(0, video_capture.get(cv2.CAP_PROP_FRAME_COUNT) - 1, frame_count, dtype=int)\n",
    "    \n",
    "    for idx in frame_indices:\n",
    "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        success, frame = video_capture.read()\n",
    "        \n",
    "        if success:\n",
    "            frame = cv2.resize(frame, frame_size)\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            # Black frame if extraction fails\n",
    "            frames.append(np.zeros((frame_size[0], frame_size[1], 3), dtype=np.float32))\n",
    "    \n",
    "    video_capture.release()\n",
    "    \n",
    "    # Normalize and ensure consistent shape\n",
    "    frames = np.array(frames, dtype=np.float32)[:frame_count] / 255.0\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce9ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path, label):\n",
    "    frames = tf.py_function(load_video_frames, [video_path], tf.float32)\n",
    "    frames.set_shape([30, 224, 224, 3])\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336945ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in df_filtered['video_file_path']:\n",
    "    if pd.isna(path) or path == '':\n",
    "        print(\"Empty video file path detected.\")\n",
    "    else:\n",
    "        print(\"Valid video path:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count empty or NaN video file paths\n",
    "empty_video_paths_count = df_filtered['video_file_path'].isna().sum() + (df_filtered['video_file_path'] == '').sum()\n",
    "\n",
    "# Print the count of empty video file paths\n",
    "print(f\"Count of empty video file paths: {empty_video_paths_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34cd541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "video_paths = df_filtered['video_file_path'].values\n",
    "labels = df_filtered['gloss_encoded'].values\n",
    "\n",
    "# Split dataset\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(paths, labels):    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    dataset = dataset.map(lambda path, label: tf.py_function(\n",
    "        func=preprocess_video, inp=[path, label], Tout=(tf.float32, tf.int32)), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=100).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce3b7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(video_paths, labels, batch_size=16, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        preprocess_video, \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(video_paths))\n",
    "    \n",
    "    # Reduced batch size to manage memory\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95cb2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tf_dataset(train_paths, train_labels, batch_size=16)\n",
    "val_dataset = create_tf_dataset(val_paths, val_labels, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52417bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_2 (Conv3D)           (None, 28, 222, 222, 32   2624      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPoolin  (None, 14, 111, 111, 32   0         \n",
      " g3D)                        )                                   \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 12, 109, 109, 32   27680     \n",
      "                             )                                   \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPoolin  (None, 6, 54, 54, 32)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 559872)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                17915936  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2000)              66000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18012240 (68.71 MB)\n",
      "Trainable params: 18012240 (68.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_sign_language_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(30, 224, 224, 3)),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "        layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "num_classes = 2000\n",
    "model = create_sign_language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35ec4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sign_language_model(input_shape=(30, 224, 224, 3), num_classes=2000):\n",
    "    model = models.Sequential([\n",
    "        # First 3D Convolutional Layer with fewer filters\n",
    "        layers.Conv3D(8, (3, 3, 3), activation='relu', input_shape=input_shape, data_format='channels_last'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "\n",
    "        # Second 3D Convolutional Layer\n",
    "        layers.Conv3D(16, (3, 3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "\n",
    "        # Third 3D Convolutional Layer with even fewer filters\n",
    "        layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "\n",
    "        # Flatten the output before passing it to Dense layers\n",
    "        layers.Flatten(),\n",
    "        \n",
    "        # Smaller Dense layer with fewer neurons\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output layer with softmax activation for multiclass classification\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Print model summary to verify architecture and parameter count\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f99ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sign_language_model(input_shape=(30, 224, 224, 3), num_classes=2000):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Reduced number of filters and layers\n",
    "        layers.Conv3D(16, (3, 3, 3), activation='relu', input_shape=input_shape, data_format='channels_last'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "        \n",
    "        layers.Conv3D(32, (3, 3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Print model summary to verify architecture\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27c2e3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 28, 222, 222, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 28, 222, 222, 8)   32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPoolin  (None, 14, 111, 111, 8)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 12, 109, 109, 16   3472      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 12, 109, 109, 16   64        \n",
      " chNormalization)            )                                   \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPoolin  (None, 6, 54, 54, 16)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 4, 52, 52, 32)     13856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 4, 52, 52, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPoolin  (None, 2, 26, 26, 32)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 43264)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2768960   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2000)              130000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2917168 (11.13 MB)\n",
      "Trainable params: 2917056 (11.13 MB)\n",
      "Non-trainable params: 112 (448.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "model = create_sign_language_model(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a263b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Limit GPU memory growth\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49fc98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(\n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27f902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[lr_reducer, early_stopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e268365a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649663da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "print( cv.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97b6e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_batch in train_dataset.take(1):\n",
    "    train_paths, train_labels = train_batch\n",
    "    print(f\"Train dataset shape: {train_paths.shape}, {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_batch in val_dataset.take(1):\n",
    "    val_paths, val_labels = val_batch\n",
    "    print(f\"Val dataset shape: {val_paths.shape}, {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for train_paths, train_labels in train_dataset.take(1):\n",
    "        print(\"Input Tensor Shape:\", train_paths.shape)\n",
    "        print(\"Label Tensor Shape:\", train_labels.shape)\n",
    "        print(\"Input Tensor Dtype:\", train_paths.dtype)\n",
    "        print(\"Label Tensor Dtype:\", train_labels.dtype)\n",
    "except Exception as e:\n",
    "    print(\"Error examining dataset:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ca63ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video_dataset(video_paths):\n",
    "    \"\"\"Analisis komprehensif dataset video\"\"\"\n",
    "    video_stats = {\n",
    "        'total_videos': len(video_paths),\n",
    "        'valid_videos': 0,\n",
    "        'invalid_videos': 0,\n",
    "        'resolution_stats': [],\n",
    "        'fps_stats': [],\n",
    "        'duration_stats': []\n",
    "    }\n",
    "\n",
    "    for video_path in video_paths:  # Sampling 100 video untuk analisis\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            \n",
    "            if not cap.isOpened():\n",
    "                video_stats['invalid_videos'] += 1\n",
    "                continue\n",
    "\n",
    "            video_stats['valid_videos'] += 1\n",
    "            \n",
    "            # Resolusi\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            video_stats['resolution_stats'].append((width, height))\n",
    "            \n",
    "            # FPS\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            video_stats['fps_stats'].append(fps)\n",
    "            \n",
    "            # Durasi\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            duration = total_frames / fps if fps > 0 else 0\n",
    "            video_stats['duration_stats'].append(duration)\n",
    "            \n",
    "            cap.release()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {video_path}: {e}\")\n",
    "            video_stats['invalid_videos'] += 1\n",
    "\n",
    "    # Statistik\n",
    "    print(\"\\n--- Analisis Dataset Video ---\")\n",
    "    print(f\"Total Video: {video_stats['total_videos']}\")\n",
    "    print(f\"Video Valid: {video_stats['valid_videos']}\")\n",
    "    print(f\"Video Invalid: {video_stats['invalid_videos']}\")\n",
    "    \n",
    "    print(\"\\nResolusi Video:\")\n",
    "    unique_resolutions = set(video_stats['resolution_stats'])\n",
    "    for res in unique_resolutions:\n",
    "        count = video_stats['resolution_stats'].count(res)\n",
    "        print(f\"{res}: {count} video\")\n",
    "    \n",
    "    print(\"\\nStatistik FPS:\")\n",
    "    print(f\"Rata-rata: {np.mean(video_stats['fps_stats']):.2f}\")\n",
    "    print(f\"Min: {np.min(video_stats['fps_stats'])}\")\n",
    "    print(f\"Max: {np.max(video_stats['fps_stats'])}\")\n",
    "    \n",
    "    print(\"\\nStatistik Durasi Video:\")\n",
    "    print(f\"Rata-rata: {np.mean(video_stats['duration_stats']):.2f} detik\")\n",
    "    print(f\"Min: {np.min(video_stats['duration_stats']):.2f} detik\")\n",
    "    print(f\"Max: {np.max(video_stats['duration_stats']):.2f} detik\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49e90f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analisis Dataset Video ---\n",
      "Total Video: 11980\n",
      "Video Valid: 11980\n",
      "Video Invalid: 0\n",
      "\n",
      "Resolusi Video:\n",
      "(720, 400): 1875 video\n",
      "(1920, 1080): 1768 video\n",
      "(656, 370): 1045 video\n",
      "(654, 480): 1 video\n",
      "(640, 480): 1824 video\n",
      "(288, 192): 2668 video\n",
      "(320, 180): 161 video\n",
      "(720, 540): 21 video\n",
      "(1280, 720): 721 video\n",
      "(736, 414): 195 video\n",
      "(640, 360): 24 video\n",
      "(320, 240): 1584 video\n",
      "(854, 480): 93 video\n",
      "\n",
      "Statistik FPS:\n",
      "Rata-rata: 28.53\n",
      "Min: 12.0\n",
      "Max: 59.94\n",
      "\n",
      "Statistik Durasi Video:\n",
      "Rata-rata: 2.43 detik\n",
      "Min: 0.63 detik\n",
      "Max: 8.12 detik\n"
     ]
    }
   ],
   "source": [
    "analyze_video_dataset(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99056122",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample labels:\")\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0174c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Sample frames (first frame of the batch):\")\n",
    "for i in range(min(5, train_paths.shape[0])):  # Print up to 5 samples\n",
    "    print(f\"Frame {i}: {train_paths[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_video_frames(video_path):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        success, _ = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frame_count += 1\n",
    "\n",
    "    video_capture.release()\n",
    "    return frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_below_30_frames = 0\n",
    "\n",
    "for video_path in df_filtered['video_file_path']:\n",
    "    frame_count = count_video_frames(video_path)\n",
    "    if frame_count < 30:\n",
    "        count_below_30_frames += 1\n",
    "\n",
    "# Menampilkan jumlah video yang memiliki kurang dari 30 frame\n",
    "print(f\"Jumlah video dengan kurang dari 30 frame: {count_below_30_frames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(corpus):\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=None,  # No limit on the number of tokens\n",
    "        output_mode='int',  # Output mode as integers\n",
    "        output_sequence_length=None,\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        ragged=True,\n",
    "        ngrams=None\n",
    "    )\n",
    "vectorizer.adapt(corpus)\n",
    "return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dbe42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(video_path, label):\n",
    "    # Konversi EagerTensor ke string path\n",
    "    video_path = video_path.numpy().decode(\"utf-8\")\n",
    "    # Baca video dan ekstrak frame\n",
    "    frames = extract_frames(video_path)\n",
    "    return frames, label\n",
    "\n",
    "def process_data(video_path, label):\n",
    "    # Konversi label dari string ke int\n",
    "    label = tf.strings.to_number(label, tf.int32)\n",
    "    # Tambahkan logika lain sesuai kebutuhan\n",
    "    return video_path, label\n",
    "\n",
    "def create_dataset(paths, labels):\n",
    "    labels = [int(label) for label in labels]  # Pastikan label berupa int\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    dataset = dataset.map(process_data)\n",
    "    return dataset\n",
    "    # Tambahkan batching dan shuffle\n",
    "    dataset = dataset.batch(8).shuffle(buffer_size=100).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "    model.add(layers.Conv3D(64, (3, 3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Contoh penggunaan\n",
    "num_frames = 30\n",
    "input_shape = (num_frames, 150, 150, 3)  # Sesuaikan dengan output extract_frames\n",
    "model = create_model(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Mendapatkan path video dan gloss dari JSON\n",
    "video_paths = []\n",
    "gloss_labels = []\n",
    "\n",
    "video_dir = './wlasl-processed/videos'\n",
    "\n",
    "for item in data:\n",
    "    gloss = item['gloss']\n",
    "    for instance in item['instances']:\n",
    "        video_id = instance['video_id']\n",
    "        video_path = os.path.join(video_dir, f'{video_id}.mp4')\n",
    "        if os.path.exists(video_path):  # Pastikan file video ada\n",
    "            video_paths.append(video_path)\n",
    "            gloss_labels.append(gloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset menjadi train dan validation\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    video_paths, gloss_labels, test_size=0.2, stratify=gloss_labels\n",
    ")\n",
    "\n",
    "# Buat dataset TensorFlow\n",
    "train_dataset = create_dataset(train_paths, train_labels)\n",
    "val_dataset = create_dataset(val_paths, val_labels)\n",
    "\n",
    "for video, label in train_dataset.take(5):  # menampilkan 5 batch pertama\n",
    "    print(\"Video batch:\", video.numpy())\n",
    "    print(\"Label batch:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total training samples: {len(train_paths)}, Total validation samples: {len(val_paths)}\")\n",
    "# Coba cetak beberapa contoh untuk memastikan data benar\n",
    "for video, label in create_dataset(train_paths[:2], train_labels[:2]).take(1):\n",
    "    print(video.shape, label)  # Video shape harus sesuai dengan input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video, label in create_dataset(train_paths[:2], train_labels[:2]).take(1):\n",
    "    print(\"Video shape:\", video.shape)  # Harus (batch_size, num_frames, height, width, channels)\n",
    "    print(\"Label shape:\", label.shape)  # Harus (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8124e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
